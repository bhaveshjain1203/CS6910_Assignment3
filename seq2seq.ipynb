{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import necessary libraries\nimport os\nimport wandb\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as Function\nimport argparse\n\n# Check if CUDA is available\nuse_cuda = torch.cuda.is_available()\n\n# Set the device type to CUDA if available, otherwise use CPU\nif use_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n  \nhidden_size = 256\ninput_lang = \"eng\"\ntarget_lang = \"hin\"\ncell_type = \"LSTM\"\nnum_layers_encoder = 2\nnum_layers_decoder = 2\ndrop_out = 0.2\nepochs = 5\nembedding_size = 256\nbi_directional = False\nbatch_size = 32\n\n  \nF=Function\nStart_Symbol, End_Symbol, Unknown, Padding = 0, 1, 2, 3\n\nclass Vocabulary:\n    def __init__(self):\n        self.char2count = {}\n        self.char2index = {}\n        self.n_chars = 4\n        self.index2char = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n\n\n    def addWord(self, word):\n        for char in word:\n            if char not in self.char2index:\n                self.char2index[char] = self.n_chars\n                self.index2char[self.n_chars] = char\n                self.char2count[char] = 1\n                self.n_chars += 1\n            else:\n                self.char2count[char] += 1\n\n# Define a function to prepare the data\ndef prepareDataWithoutAttn(dir):\n    # Read the CSV file into a DataFrame with columns \"input\" and \"target\"\n    data = pd.read_csv(dir, sep=\",\", names=[\"input\", \"target\"])\n\n    # Find the maximum length of input and target sequences\n    # max_input_length = max([len(txt) for txt in data[\"input\"].to_list()])\n    max_input_length = 0\n    for txt in data[\"input\"].to_list():\n        max_input_length = max(max_input_length, len(txt))\n    \n    max_target_length = 0\n    for txt in data[\"target\"].to_list():\n        max_target_length = max(max_target_length, len(txt))\n    \n    max_len=0\n    if max_input_length > max_target_length:\n        max_len = max_input_length\n    else:\n        max_len = max_target_length\n    # max(max_input_length,max_target_length)\n\n    # Create Vocabulary objects for input and output languages\n    input_lang = Vocabulary()\n    output_lang = Vocabulary()\n\n    # Create pairs of input and target sequences\n    pairs = []\n    input_list, target_list = data[\"input\"].to_list(), data[\"target\"].to_list()\n    for i in range(len(input_list)):\n        pairs.append([input_list[i], target_list[i]])\n\n    # Add words to the respective vocabularies\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    # Create a dictionary containing prepared data\n    # prepared_data = {\n    #     \"input_lang\": input_lang,\n    #     \"output_lang\": output_lang,\n    #     \"pairs\": pairs,\n    #     \"max_len\": max_len\n    # }\n\n    return input_lang,output_lang,pairs,max_len\n\n# Define a helper function to convert a word to a tensor\ndef helpTensorWithoutAttn(lang, word, max_length):\n    index_list = []\n    for char in word:\n        if char in lang.char2index.keys():\n            index_list.append(lang.char2index[char])\n        else:\n            index_list.append(Unknown)\n    indexes = index_list\n    indexes.append(End_Symbol)\n    indexes.extend([Padding] * (max_length - len(indexes)))\n    result = torch.LongTensor(indexes)\n    if use_cuda:\n        return result.cuda()\n    else:\n        return result\n\n# Define a function to convert pairs of input and target sequences to tensors\ndef MakeTensorWithoutAttn(input_lang, output_lang, pairs, reach):\n    res = []\n    for pair in pairs:\n        # Convert input and target sequences to tensors using the helpTensorWithoutAttn function\n        input_variable = helpTensorWithoutAttn(input_lang, pair[0], reach)\n        target_variable = helpTensorWithoutAttn(output_lang, pair[1], reach)\n        res.append((input_variable, target_variable))\n    return res\n\n#Encoder Class\nclass EncoderRNNWithoutAttn(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n        super(EncoderRNNWithoutAttn, self).__init__()\n\n        # Initialize the EncoderRNNWithoutAttn with the provided parameters\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_encoder = num_layers_encoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        # Create the specified cell layer (RNN, GRU, or LSTM)\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_encoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        # Apply dropout to the embedded input sequence\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n\n        # Pass the embedded input through the cell layer\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        # Initialize the hidden state with zeros\n        res = torch.zeros(num_layers_enc * 2 if self.bi_directional else num_layers_enc, batch_size, self.hidden_size)\n\n        # Move the hidden state to the GPU if use_cuda is True, else return as is\n        return res.cuda() if use_cuda else res\n\n#Decoder class\nclass DecoderRNNWithoutAttn(nn.Module):\n    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n        super(DecoderRNNWithoutAttn, self).__init__()\n\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_decoder = num_layers_decoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(output_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        # Create the specified cell layer (RNN, GRU, or LSTM)\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_decoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n        # Linear layer for output\n        self.out = nn.Linear(\n            self.hidden_size * 2 if self.bi_directional else self.hidden_size,\n            output_size,\n        )\n\n        # Softmax activation\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, batch_size, hidden):\n        # Apply dropout to the embedded input sequence and pass it through the cell layer\n        output = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n        output, hidden = self.cell_layer(output, hidden)\n\n        # Apply softmax activation to the output\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n# Function to calculate loss (if is_training then training loss else validation loss)\ndef calc_lossWithoutAttn(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n    # Initialize the encoder hidden state\n    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    # Check if LSTM and initialize cell state\n    if cell_type == \"LSTM\":\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        output_hidden = (output_hidden, encoder_cell_state)\n\n    # Zero the gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Get input and target sequence lengths\n    # input_length = input_tensor.size(0)\n    # target_length = target_tensor.size(0)\n\n    # Initialize loss\n    loss = 0\n\n    # Encoder forward pass\n    for ei in range(input_tensor.size(0)):\n        output_hidden = encoder(input_tensor[ei], batch_size, output_hidden)[1]\n\n    # Initialize decoder input\n    decoder_input = torch.LongTensor([Start_Symbol] * batch_size)\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    # Determine if using teacher forcing\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Loop over target sequence\n    if is_training:\n        # Training phase\n        for di in range(target_tensor.size(0)):\n            decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n            loss = criterion(decoder_output, target_tensor[di]) + loss\n    else:\n        # Validation phase\n        with torch.no_grad():\n            for di in range(target_tensor.size(0)):\n                decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = decoder_output.argmax(dim=1)\n\n    # Backpropagation and optimization in training phase\n    if is_training:\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n    # Return the average loss per target length\n    return loss.item() / target_tensor.size(0)\n\n\n# Calculate the accuracyWithoutAttn of the Seq2SeqWithoutAttn model\ndef accuracyWithoutAttn(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang):\n    with torch.no_grad():\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n            # Initialize encoder hidden state\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            # Check if LSTM and initialize cell state\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            # input_length = input_variable.size()[0]\n            # target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_variable.size()[0], batch_size)\n\n            # Initialize encoder outputs\n            # encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n            # encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            # Encoder forward pass\n            for ei in range(input_variable.size()[0]):\n                encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)[1]\n\n            decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            # Decoder forward pass\n            for di in range(target_variable.size()[0]):\n                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n                topi = decoder_output.data.topk(1)[1]\n                output[di] = torch.cat(tuple(topi))\n                decoder_input = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n\n            # Calculate accuracyWithoutAttn\n            for di in range(output.size()[0]):\n                ignore = [Start_Symbol, End_Symbol, Padding]\n                sent = [output_lang.index2char[letter.item()] for letter in output[di] if letter not in ignore]\n                y = [output_lang.index2char[letter.item()] for letter in batch_y[di] if letter not in ignore]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n# Train and evaluate the Seq2SeqWithoutAttn model\ndef seq2seqWithoutAttn(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang):\n    max_length = max_length_word - 1\n    # Define the optimizer and criterion\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total = 0\n        val_loss_total = 0\n\n        # Training phase\n        for batch_x, batch_y in train_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the training loss\n            loss = calc_lossWithoutAttn(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n\n        # Validation phase\n        for batch_x, batch_y in val_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the validation loss\n            loss = calc_lossWithoutAttn(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n\n        # Calculate validation accuracyWithoutAttn\n        val_acc = accuracyWithoutAttn(encoder, decoder, val_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n        val_acc /= 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        \n        if epochs-1==epoch :\n            test_acc = accuracyWithoutAttn(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n            test_acc /= 100\n            print(f\"Test Accuracy: {test_acc:.4%}\")\n            \n\n\ndef prepareData(dir):\n\n    input_lang = Vocabulary()\n    output_lang = Vocabulary()\n\n    data = pd.read_csv(dir, sep=\",\", names=[\"input\", \"target\"])\n\n    input_list = data[\"input\"].to_list()\n    target_list = data[\"target\"].to_list()\n\n    max_target_length = max([len(txt) for txt in data[\"target\"].to_list()])\n\n    pairs = []\n    for i in range(len(target_list)):\n        pairs.append([input_list[i], target_list[i]])\n\n    max_input_length = max([len(txt) for txt in data[\"input\"].to_list()])\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    prepared_data = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_input_length\": max_input_length,\n        \"max_target_length\": max_target_length,\n    }\n\n    return prepared_data\n\ndef helpindex(lang, word):\n    l=[]\n    for i in range(len(word)):\n        if word[i] not in lang.char2index.keys():\n            l.append(Unknown)\n        else:\n            l.append(lang.char2index[word[i]])\n    return l\n\ndef helpTensor(lang, word, max_length):\n    indexes = helpindex(lang, word)\n    indexes.append(End_Symbol)\n    indexes.extend([Padding] * (max_length - len(indexes)))\n    result = torch.LongTensor(indexes)\n    if use_cuda==False:\n        return result\n    else:\n        return result.cuda()\n\ndef MakeTensor(input_lang, output_lang, pairs, max_length):\n    res = []\n    for pair in pairs:\n        input_variable = helpTensor(input_lang, pair[0], max_length)\n        target_variable = helpTensor(output_lang, pair[1], max_length)\n        res.append((input_variable, target_variable))\n    return res\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size,hidden_size,num_layers_encoder,cell_type,drop_out,bi_directional):\n        super(EncoderRNN, self).__init__()\n\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_encoder = num_layers_encoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_encoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        res = torch.zeros(\n            num_layers_enc * 2 if self.bi_directional else num_layers_enc,\n            batch_size,\n            self.hidden_size,\n        )\n        if use_cuda== False:\n            return res\n        else:\n            return res.cuda()\n\n\n\nclass DecoderAttention(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        embedding_size,\n        cell_type,\n        num_layers_decoder,\n        drop_out,\n        max_length_word,\n        output_size,\n    ):\n\n        super(DecoderAttention, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n        self.cell_type = cell_type\n        self.num_layers_decoder = num_layers_decoder\n        self.drop_out = drop_out\n        self.max_length_word = max_length_word\n\n        self.embedding = nn.Embedding(output_size, embedding_dim=self.embedding_size)\n        self.attention_layer = nn.Linear(\n            self.embedding_size + self.hidden_size, self.max_length_word\n        )\n        self.attention_combine = nn.Linear(\n            self.embedding_size + self.hidden_size, self.embedding_size\n        )\n        self.dropout = nn.Dropout(self.drop_out)\n\n        self.cell_layer = None\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n\n        if self.cell_type in cell_map:\n            self.cell_layer = cell_map[self.cell_type](\n                self.embedding_size,\n                self.hidden_size,\n                num_layers=self.num_layers_decoder,\n                dropout=self.drop_out,\n            )\n\n        self.out = nn.Linear(self.hidden_size, output_size)\n\n    def forward(self, input, batch_size, hidden, encoder_outputs):\n\n        embedded = self.embedding(input).view(1, batch_size, -1)\n\n        attention_weights = None\n        if self.cell_type == \"LSTM\":\n            attention_weights = Function.softmax(\n                self.attention_layer(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1\n            )\n\n        else:\n            attention_weights = Function.softmax(\n                self.attention_layer(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n            )\n\n        attention_applied = torch.bmm(\n            attention_weights.view(batch_size, 1, self.max_length_word),\n            encoder_outputs,\n        ).view(1, batch_size, -1)\n        output = torch.cat((embedded[0], attention_applied[0]), 1)\n        output = self.attention_combine(output).unsqueeze(0)\n        output = Function.relu(output)\n        # if self.cell_type=RNN\" :\n        output, hidden = self.cell_layer(output, hidden)\n        output = Function.log_softmax(self.out(output[0]), dim=1)\n\n        return output, hidden, attention_weights\n\n\n\ndef train_and_val_with_attn(\n    encoder,\n    decoder,\n    encoder_optimizer,\n    decoder_optimizer,\n    input_tensor,\n    target_tensor,\n    criterion,\n    batch_size,\n    cell_type,\n    num_layers_enc,\n    max_length,is_training,\n    teacher_forcing_ratio=0.5,\n):\n\n    encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    if cell_type == \"LSTM\":\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n    if is_training:\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], batch_size, encoder_hidden\n        )\n        encoder_outputs[ei] = encoder_output[0]\n\n    decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    decoder_hidden = encoder_hidden\n    if is_training:\n        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        if use_teacher_forcing == False:\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n                )\n                #2 for loop ko bhar dal de\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n\n                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n                loss += criterion(decoder_output, target_tensor[di])\n        else:\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n                )\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = target_tensor[di]\n            \n\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n    else :\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input,\n                batch_size,\n                decoder_hidden,\n                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n            )\n            topv, topi = decoder_output.data.topk(1)\n            decoder_input = torch.cat(tuple(topi))\n\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n            loss += criterion(decoder_output, target_tensor[di])\n\n\n    return loss.item() / target_length\n\n\n# batch_size,num_layers_enc,cell_type,output_lang,criterion,\ndef accuracy_with_attention(\n    encoder,\n    decoder,\n    loader,\n    batch_size,\n    num_layers_enc,\n    cell_type,\n    output_lang,\n    criterion,\n    max_length,\n):\n\n    with torch.no_grad():\n\n        # batch_size = configuration[\"batch_size\"]\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            input_length = input_variable.size()[0]\n            target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_length, batch_size)\n\n            encoder_outputs = Variable(\n                torch.zeros(max_length, batch_size, encoder.hidden_size)\n            )\n            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(\n                    input_variable[ei], batch_size, encoder_hidden\n                )\n                encoder_outputs[ei] = encoder_output[0]\n\n            decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(\n                        batch_size, max_length, encoder.hidden_size\n                    ),\n                )\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n                output[di] = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n            for di in range(output.size()[0]):\n                ignore = [Start_Symbol, End_Symbol, Padding]\n                sent = [\n                    output_lang.index2char[letter.item()]\n                    for letter in output[di]\n                    if letter not in ignore\n                ]\n                y = [\n                    output_lang.index2char[letter.item()]\n                    for letter in batch_y[di]\n                    if letter not in ignore\n                ]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n\ndef cal_val_loss_with_attn(\n    encoder,\n    decoder,\n    input_tensor,\n    target_tensor,\n    batch_size,\n    criterion,\n    cell_type,\n    num_layers_enc,\n    max_length,\n):\n\n    with torch.no_grad():\n\n        encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n        if cell_type == \"LSTM\":\n            encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n            encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n        input_length = input_tensor.size()[0]\n        target_length = target_tensor.size()[0]\n\n        encoder_outputs = Variable(\n            torch.zeros(max_length, batch_size, encoder.hidden_size)\n        )\n        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n        loss = 0\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(\n                input_tensor[ei], batch_size, encoder_hidden\n            )\n            encoder_outputs[ei] = encoder_output[0]\n\n        decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n        if use_cuda== True:\n            decoder_input = decoder_input.cuda()  \n        else :\n            decoder_input = decoder_input\n\n        decoder_hidden = encoder_hidden\n\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input,\n                batch_size,\n                decoder_hidden,\n                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n            )\n            topv, topi = decoder_output.data.topk(1)\n            decoder_input = torch.cat(tuple(topi))\n\n            if use_cuda== True:\n                decoder_input = decoder_input.cuda()  \n            else :\n                decoder_input = decoder_input\n            loss += criterion(decoder_output, target_tensor[di])\n\n    return loss.item() / target_length\n\n\ndef Attention_seq2seq(\n    encoder,\n    decoder,\n    train_loader,\n    val_loader,\n    test_loader,\n    learning_rate,\n    optimizer,\n    epochs,\n    max_length_word,\n    attention,\n    num_layers_enc,\n    output_lang,\n):\n    max_length = max_length_word - 1\n    encoder_optimizer = (\n        optim.NAdam(encoder.parameters(), lr=learning_rate)\n        if optimizer == \"nadam\"\n        else optim.Adam(encoder.parameters(), lr=learning_rate)\n    )\n    decoder_optimizer = (\n        optim.NAdam(decoder.parameters(), lr=learning_rate)\n        if optimizer == \"nadam\"\n        else optim.Adam(decoder.parameters(), lr=learning_rate)\n    )\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total, val_loss_total  =0, 0\n        \n        for batchx, batchy in train_loader:\n            batchx = Variable(batchx.transpose(0, 1))\n            batchy = Variable(batchy.transpose(0, 1))\n            loss = train_and_val_with_attn(\n                encoder,\n                decoder,\n                encoder_optimizer,\n                decoder_optimizer,\n                batchx,\n                batchy,\n                criterion,\n                batch_size,\n                cell_type,\n                num_layers_enc,\n                max_length + 1,\n                True, #is_training\n            )\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} | \", end=\"\")\n\n        for batchx, batchy in val_loader:\n            batchx = Variable(batchx.transpose(0, 1))\n            batchy = Variable(batchy.transpose(0, 1))\n            loss = train_and_val_with_attn(\n                encoder,\n                decoder,\n                encoder_optimizer,\n                decoder_optimizer,\n                batchx,\n                batchy,\n                criterion,\n                batch_size,\n                cell_type,\n                num_layers_enc,\n                max_length + 1,\n                False,#is_training=\n            )\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} | \", end=\"\")\n        val_acc = accuracy_with_attention(\n            encoder,\n            decoder,\n            val_loader,\n            batch_size,\n            num_layers_enc,\n            cell_type,\n            output_lang,\n            criterion,\n            max_length + 1,\n        )\n        val_acc = val_acc / 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        if epochs-1==epoch:\n            test_acc = accuracy_with_attention(\n            encoder,\n            decoder,\n            test_loader,\n            batch_size,\n            num_layers_enc,\n            cell_type,\n            output_lang,\n            criterion,\n            max_length + 1,\n        )\n            test_acc = test_acc / 100\n            print(f\"Test Accuracy: {test_acc:.4%}\")\n\ndef to_dict(input_lang,output_lang,pairs,max_len):\n    d = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_len\": max_len\n    }\n    return d\n\ndef main(flag):\n    teacher_forcing_ratio = 0.5\n    optimizer = \"Nadam\"\n    learning_rate = 0.001\n    train_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv\"\n    validation_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv\"\n    test_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv\"\n\n\n    if flag:\n        train_prepared_data = prepareData(train_path)\n        input_langs, output_langs, pairs = (\n            train_prepared_data[\"input_lang\"],\n            train_prepared_data[\"output_lang\"],\n            train_prepared_data[\"pairs\"],\n        )\n        print(\"train:sample:\", random.choice(pairs))\n        print(f\"Number of training examples: {len(pairs)}\")\n\n        max_input_length, max_target_length = (\n            train_prepared_data[\"max_input_length\"],\n            train_prepared_data[\"max_target_length\"],\n        )\n\n        # validation\n        val_prepared_data = prepareData(validation_path)\n        val_pairs = val_prepared_data[\"pairs\"]\n        print(\"validation:sample:\", random.choice(val_pairs))\n        print(f\"Number of validation examples: {len(val_pairs)}\")\n        # Test\n        max_input_length_val, max_target_length_val = (\n            val_prepared_data[\"max_input_length\"],\n            val_prepared_data[\"max_target_length\"],\n        )\n        test_prepared_data = prepareData(validation_path)\n        test_pairs = test_prepared_data[\"pairs\"]\n        print(\"Test:sample:\", random.choice(test_pairs))\n        print(f\"Number of Test examples: {len(test_pairs)}\")\n\n        max_input_length_test, max_target_length_test = (\n            test_prepared_data[\"max_input_length\"],\n            test_prepared_data[\"max_target_length\"],\n        )\n        max_len_all = (\n            max(\n                max_input_length,\n                max_target_length,\n                max_input_length_val,\n                max_target_length_val,\n                max_input_length_test,\n                max_target_length_test,\n            )\n            + 1\n        )\n\n        max_len = max(max_input_length, max_target_length) + 3\n        print(max_len)\n\n        pairs = MakeTensor(input_langs, output_langs, pairs, max_len)\n        val_pairs = MakeTensor(input_langs, output_langs, val_pairs, max_len)\n        test_pairs = MakeTensor(input_langs, output_langs, test_pairs, max_len)\n\n        train_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_pairs, batch_size=batch_size, shuffle=True)\n\n        encoder1 = EncoderRNN(\n            input_langs.n_chars,\n            embedding_size,\n            hidden_size,\n            num_layers_encoder,\n            cell_type,\n            drop_out,\n            bi_directional,\n        )\n        attndecoder1 = DecoderAttention(\n            hidden_size,\n            embedding_size,\n            cell_type,\n            num_layers_decoder,\n            drop_out,\n            max_len,\n            output_langs.n_chars,\n        )\n        if use_cuda== True:\n            encoder1 = encoder1.cuda()\n            attndecoder1 = attndecoder1.cuda()\n        print(\"with attention\")\n        attention = True\n        Attention_seq2seq(\n            encoder1,\n            attndecoder1,\n            train_loader,\n            val_loader,\n            test_loader,\n            learning_rate,\n            optimizer,\n            epochs,\n            max_len,\n            attention,\n            num_layers_encoder,\n            output_langs,\n        )\n    else:\n        # Prepare training data\n        _input_lang,_output_lang,_pairs,_max_len = prepareDataWithoutAttn(train_path)\n        train_prepared_data = to_dict(_input_lang,_output_lang,_pairs,_max_len)\n        input_langs, output_langs, pairs = train_prepared_data[\"input_lang\"], train_prepared_data[\"output_lang\"], train_prepared_data[\"pairs\"]\n        print(\"train:sample:\", random.choice(pairs))\n        print(f\"Number of training examples: {len(pairs)}\")\n        max_len = train_prepared_data[\"max_len\"]\n\n        # Prepare validation data\n        _input_lang,_output_lang,_pairs,_max_len = prepareDataWithoutAttn(validation_path)\n        val_prepared_data = to_dict(_input_lang,_output_lang,_pairs,_max_len)\n        val_pairs = val_prepared_data[\"pairs\"]\n        print(\"validation:sample:\", random.choice(val_pairs))\n        print(f\"Number of validation examples: {len(val_pairs)}\")\n        max_len_val = val_prepared_data[\"max_len\"]\n\n        # Prepare test data\n        _input_lang,_output_lang,_pairs,_max_len = prepareDataWithoutAttn(test_path)\n        test_prepared_data = to_dict(_input_lang,_output_lang,_pairs,_max_len)\n        test_pairs = test_prepared_data[\"pairs\"]\n        print(\"Test:sample:\", random.choice(test_pairs))\n        print(f\"Number of Test examples: {len(test_pairs)}\")\n\n        max_len_test = test_prepared_data[\"max_len\"]\n        max_len = max(max_len, max_len_val, max_len_test) + 4\n        print(max_len)\n\n        # Convert data to tensors and create data loaders\n        pairs = MakeTensorWithoutAttn(input_langs, output_langs, pairs, max_len)\n        val_pairs = MakeTensorWithoutAttn(input_langs, output_langs, val_pairs, max_len)\n        test_pairs = MakeTensorWithoutAttn(input_langs, output_langs, test_pairs, max_len)\n\n        train_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_pairs, batch_size=batch_size, shuffle=True)\n\n        # Create the encoder and decoder models\n        encoder1 = EncoderRNNWithoutAttn(input_langs.n_chars, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional)\n        decoder1 = DecoderRNNWithoutAttn(embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional, output_langs.n_chars)\n\n        if use_cuda:\n            encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n\n        print(\"vanilla seq2seqWithoutAttn\")\n        # Train and evaluate the Seq2SeqWithoutAttn model\n        seq2seqWithoutAttn(encoder1, decoder1, train_loader, val_loader, test_loader, learning_rate, optimizer, epochs, max_len, num_layers_encoder, output_langs)\n\nattention_flag=False\nmain(attention_flag)\nattention_flag=True\nmain(attention_flag)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T18:07:33.388836Z","iopub.execute_input":"2023-05-21T18:07:33.389223Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"train:sample: ['ashubhon', 'अशुभों']\nNumber of training examples: 51200\nvalidation:sample: ['tapaswini', 'तपस्विनी']\nNumber of validation examples: 4096\nTest:sample: ['mtech', 'एमटेक']\nNumber of Test examples: 4096\n30\nvanilla seq2seqWithoutAttn\nEpoch: 0 | Train Loss: 0.7844 |Val Loss: 0.4975 |Val Accuracy: 7.2266%\n","output_type":"stream"}]}]}